{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dbb6b53-f24e-43fe-b9a9-4f3614b4f96b",
   "metadata": {},
   "source": [
    "# Fully-connected neural network\n",
    "\n",
    "This notebook implements a precipitation retrieval for the IPWG SPR dataset based on a fully-connected neural network implemented using PyTorch and Lightning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356017b8-fbc9-4755-9108-a85f3470fd0d",
   "metadata": {},
   "source": [
    "## The training data\n",
    "\n",
    "The ``ipwgml`` package provides dataset classes that take care of downloading, preprocessing and loading of the SPR training data. The ``ipwgml.pytorch.dataset.SPRTabular`` class implements the PyTorch dataset interface for the SPR data in tabular format, i.e., for pixel-based retrievals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8015ca-c8cd-4bfd-bc2c-2677a16edbe1",
   "metadata": {},
   "source": [
    "The retrieval input data to load and the quality criteria for the reference surface precipitation data can be configured using the ``InputConfig`` classes from the ``ipwgml.input`` module and the ``TargetConfig`` class from the ``ipgml.target`` module, respectively.\n",
    "\n",
    "For this example notebook we use only GMI observations as input. We configure the ``GMI`` input to not include the earth-incidence angles. Moreover, enable minimum-maximum normalization of the input and replace NAN values with -1.5.\n",
    "\n",
    "> **NOTE**: Both input and reference data in the SPR dataset may contain NANs because observations or precipitation estimates may be missing or of insufficient quality. It is up to the user to handle those.\n",
    "\n",
    "We choose the ``on_swath`` geometry for the retrieval, which is a natural choice for pixel-based retrievals. We also set up batching in the dataset, which is more efficient for tabular data than leaving it to the PyTorch data loader to perform the batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30887ffe-c08d-4f9c-b8ba-9cf5edb61486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipwgml.input import GMI, Ancillary, Geo, GeoIR\n",
    "from ipwgml.target import TargetConfig\n",
    "\n",
    "target_config = TargetConfig(min_rqi=0.5)\n",
    "inputs = [GMI(include_angles=False, normalize=\"minmax\", nan=-1.5)]\n",
    "geometry = \"on_swath\"\n",
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e12e6b9-7076-45ea-a441-e076ccb3a152",
   "metadata": {},
   "source": [
    "With these settings, we can instantiate the training data dataset. By setting ``stack=True`` we also tell the retrieval to stack all input tensors instead of loading the input data as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e679abf7-6dfc-421c-9c2d-f8eac00740e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from ipwgml.pytorch.datasets import SPRTabular\n",
    "\n",
    "training_data = SPRTabular(\n",
    "    sensor=\"gmi\",\n",
    "    geometry=geometry,\n",
    "    split=\"training\",\n",
    "    retrieval_input=inputs,\n",
    "    batch_size=batch_size,\n",
    "    target_config=target_config,\n",
    "    stack=True,\n",
    "    download=True\n",
    ")\n",
    "training_loader = DataLoader(training_data, shuffle=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdef0ac-d153-407a-9131-0ae0ab6c44ed",
   "metadata": {},
   "source": [
    "We the above configuration, the training data loaded by the ``training_loader`` has the following dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98456697-5599-4030-b565-bf336f7d9315",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt, target = next(iter(training_loader))\n",
    "print(\"Input tensor shape: \", inpt.shape)\n",
    "print(\"Target tensor shape: \", target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479cc1bc-bbc5-4733-8142-05e2d8db204a",
   "metadata": {},
   "source": [
    "We also create a validation loader with the same configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5eda21-d394-4408-afb0-d107db88e8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = SPRTabular(\n",
    "    sensor=\"gmi\",\n",
    "    geometry=\"on_swath\",\n",
    "    split=\"validation\",\n",
    "    retrieval_input=inputs,\n",
    "    batch_size=batch_size,\n",
    "    target_config=target_config,\n",
    "    stack=True,\n",
    "    download=True\n",
    ")\n",
    "validation_loader = DataLoader(validation_data, shuffle=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0279ec64-d5ab-4afe-a9d8-bed4c8a618a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits\n",
    "import lightning as L\n",
    "\n",
    "OUTPUTS = [\n",
    "    \"surface_precip\",\n",
    "    \"probability_of_precipitation\",\n",
    "    \"probability_of_heavy_precipitation\"\n",
    "]\n",
    "\n",
    "N_EPOCHS = 20\n",
    "\n",
    "class MLP(L.LightningModule):\n",
    "    \"\"\"\n",
    "    Lightning module implementing a multi-layer perceptron (MLP) for retrieving precipitation from satellite\n",
    "    observations.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input_features: int,\n",
    "        n_hidden_layers: int,\n",
    "        n_neurons: int,\n",
    "        activation_fn: Callable[[], nn.Module] = nn.ReLU,\n",
    "        normalization_layer: Callable[[int], nn.Module] = nn.LayerNorm\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_input_features: The number of features in the input\n",
    "            n_hidden_layers: The number of hidden layers in the MLP\n",
    "            n_neurons: The number of neurons in the hidden layers\n",
    "            activation_fn: A callable to create activation function layers.\n",
    "            normalization_layer: A callable to create normalization layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        blocks = [\n",
    "            nn.Linear(n_input_features, n_neurons),\n",
    "            activation_fn(),\n",
    "            normalization_layer(n_neurons)\n",
    "        ]\n",
    "        for _ in range(n_hidden_layers):\n",
    "            blocks += [\n",
    "                nn.Linear(n_neurons, n_neurons),\n",
    "                activation_fn(),\n",
    "                normalization_layer(n_neurons)\n",
    "            ]\n",
    "        self.body = nn.Sequential(*blocks)\n",
    "\n",
    "        heads = {}\n",
    "        for output in OUTPUTS:\n",
    "            heads[output] =  nn.Sequential(\n",
    "                nn.Linear(n_neurons, n_neurons),\n",
    "                activation_fn(),\n",
    "                normalization_layer(n_neurons),\n",
    "                nn.Linear(n_neurons, 1)\n",
    "            )\n",
    "        self.heads = nn.ModuleDict(heads)\n",
    "\n",
    "    def forward(self, retrieval_input: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward retrieval input through network and produce dictionary with predictions.\n",
    "\n",
    "        Args:\n",
    "            retrieval_input: The retrieval input as a single torch.Tensor.\n",
    "\n",
    "        Return:\n",
    "            A dictionary containing the predictions for 'surface_precip', 'probability_of_precipitation',\n",
    "            and 'probability_of_heavy_precipitation'.\n",
    "        \"\"\"\n",
    "        y = self.body(retrieval_input)\n",
    "        return {\n",
    "            name: module(y) for name, module in self.heads.items()\n",
    "        }\n",
    "        \n",
    "    def training_step(self, batch, batch_idx) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculates the loss-function gradients for the MLP.\n",
    "\n",
    "        The loss is calculated as the sum of the MSE for 'surface_precip' and the binary cross-entropy loss\n",
    "        for precipitation detection and heavy precipitation detection.\n",
    "\n",
    "        Args:\n",
    "            batch: A tuple containing the training data loaded from the data loader.\n",
    "            batch_idx: The index of the batch in the current epoch. Not used.\n",
    "\n",
    "        Return:\n",
    "            A scalar torch.Tensor containing the total loss.\n",
    "        \"\"\"\n",
    "        inpt, surface_precip = batch\n",
    "        pred = self(inpt)\n",
    "\n",
    "        valid = torch.isfinite(surface_precip)\n",
    "        surface_precip = surface_precip[valid]\n",
    "        precip_mask = (surface_precip > 1e-3).to(dtype=torch.float32)\n",
    "        heavy_precip_mask = (surface_precip > 10).to(dtype=torch.float32)\n",
    "        surface_precip_pred = pred[\"surface_precip\"][valid]\n",
    "        pop = pred[\"probability_of_precipitation\"][valid]\n",
    "        pohp = pred[\"probability_of_heavy_precipitation\"][valid]\n",
    "        \n",
    "        # MSE loss for QPE\n",
    "        loss_estim = ((surface_precip_pred[..., 0] - surface_precip) ** 2).mean()\n",
    "        # BCE loss for detection targets\n",
    "        loss_detect = binary_cross_entropy_with_logits(pop[..., 0], precip_mask)\n",
    "        loss_detect_heavy = binary_cross_entropy_with_logits(pop[..., 0], heavy_precip_mask)\n",
    "        tot_loss =  loss_estim + loss_detect + loss_detect_heavy\n",
    "        self.log(\"loss\", loss_estim, prog_bar=True)\n",
    "        return tot_loss\n",
    "\n",
    "    def validation(self, batch, batch_idx) -> None:\n",
    "        \"\"\"\n",
    "        Calculates the loss-function values on validation data.\n",
    "\n",
    "        Args:\n",
    "            batch: A tuple containing the training data loaded from the data loader.\n",
    "            batch_idx: The index of the batch in the current epoch. Not used.\n",
    "        \"\"\"\n",
    "        inpt, surface_precip = batch\n",
    "        pred = self(inpt)\n",
    "\n",
    "        valid = torch.isfinite(surface_precip)\n",
    "        surface_precip = surface_precip[valid]\n",
    "        precip_mask = (surface_precip > 1e-3).to(dtype=torch.float32)\n",
    "        heavy_precip_mask = (surface_precip > 10).to(dtype=torch.float32)\n",
    "        surface_precip_pred = pred[\"surface_precip\"][valid]\n",
    "        pop = pred[\"probability_of_precipitation\"][valid]\n",
    "        pohp = pred[\"probability_of_heavy_precipitation\"][valid]\n",
    "        \n",
    "        # MSE loss for QPE\n",
    "        loss_estim = ((surface_precip_pred[..., 0] - surface_precip) ** 2).mean()\n",
    "        # BCE loss for detection targets\n",
    "        loss_detect = binary_cross_entropy_with_logits(pop[..., 0], precip_mask)\n",
    "        loss_detect_heavy = binary_cross_entropy_with_logits(pop[..., 0], heavy_precip_mask)\n",
    "        tot_loss =  loss_estim + loss_detect + loss_detect_heavy\n",
    "        self.log(\"loss_estim\", loss_estim, prog_bar=True)\n",
    "        self.log(\"loss_detect\", loss_detect, prog_bar=True)\n",
    "        self.log(\"loss_detect_heavy\", loss_detect, prog_bar=True)\n",
    "    \n",
    "    def configure_optimizers(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        We use the Adam optimizer with a cosine annealing learning rate schedule.\n",
    "        \"\"\"\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = N_EPOCHS)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": scheduler\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e42aaba-5d99-40b9-81e1-1aa0e020f6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(n_input_features=13, n_hidden_layers=4, n_neurons=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abe1ccb-6fac-4e41-8613-684fd7317677",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(max_epochs=N_EPOCHS)\n",
    "trainer.fit(model=mlp, train_dataloaders=training_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33ff035-fff2-4d35-ba80-bb1c04997aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipwgml.evaluation import Evaluator\n",
    "evaluator = Evaluator(\n",
    "    sensor=\"gmi\",\n",
    "    geometry=\"on_swath\",\n",
    "    retrieval_input=[{\"name\": \"gmi\", \"normalize\": \"minmax\", \"nan\": -1.5}],\n",
    "    download=False\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3b25e4-57c6-4ad3-823a-d8aa0e79e404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "mlp = mlp.eval()\n",
    "\n",
    "def retrieval_fn(retrieval_input: xr.Dataset):\n",
    "    obs_gmi = torch.tensor(retrieval_input.obs_gmi.data)\n",
    "    with torch.no_grad():\n",
    "        pred = mlp(obs_gmi.T)\n",
    "        dims = (\"samples\",)\n",
    "        results = xr.Dataset({\n",
    "            \"surface_precip\": (dims, pred[\"surface_precip\"].cpu().numpy()[..., 0]),\n",
    "            \"probability_of_precipitation\": (dims, nn.functional.sigmoid(pred[\"probability_of_precipitation\"][..., 0]).cpu().numpy()),\n",
    "            \"probability_of_heavy_precipitation\": (dims, nn.functional.sigmoid(pred[\"probability_of_heavy_precipitation\"][..., 0]).cpu().numpy()),\n",
    "        })\n",
    "    return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53652d20-6a50-49fc-a994-bf5613dc98bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.plot_retrieval_results(75, retrieval_fn, input_data_format=\"tabular\", batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feecab9b-0041-4d77-9f5b-fe6c9a55969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate(retrieval_fn=retrieval_fn, input_data_format=\"tabular\", batch_size=4048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2539f82-732f-4f9a-8f8d-bff9bfe0943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.get_precipitation_estimation_results(name=\"MLP (GMI)\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c1d399-c41e-4a24-acf8-5290d61e7f5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
