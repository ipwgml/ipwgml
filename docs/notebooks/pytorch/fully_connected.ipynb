{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dbb6b53-f24e-43fe-b9a9-4f3614b4f96b",
   "metadata": {},
   "source": [
    "# Fully-connected neural network retrieval\n",
    "\n",
    "This notebook implements a precipitation retrieval for the IPWG SPR dataset based on a convolutional neural network implemented using PyTorch and Lightning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdd2f2a-1655-493e-90ee-c382f0f87525",
   "metadata": {},
   "source": [
    "> **NOTE**: This notebook can be run on Google Colab. To install the necessary dependencies uncomment the following cell and execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "915e8a60-43ad-4037-b085-9af9e99f72d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ipwgml[complete]@git+https://github.com/simonpf/ipwgml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356017b8-fbc9-4755-9108-a85f3470fd0d",
   "metadata": {},
   "source": [
    "## The training data\n",
    "\n",
    "The ``ipwgml`` package provides dataset classes that take care of downloading, preprocessing and loading of the SPR training data. The ``ipwgml.pytorch.dataset.SPRTabular`` class implements the PyTorch dataset interface for the SPR data in tabular format, i.e., for pixel-based retrievals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8015ca-c8cd-4bfd-bc2c-2677a16edbe1",
   "metadata": {},
   "source": [
    "The retrieval input data to load and the quality criteria for the reference surface precipitation data can be configured using the ``InputConfig`` classes from the ``ipwgml.input`` module and the ``TargetConfig`` class from the ``ipgml.target`` module, respectively.\n",
    "\n",
    "For this example notebook we use only GMI observations as input. We configure the ``GMI`` input to not include the earth-incidence angles. Moreover, enable minimum-maximum normalization of the input and replace NAN values with -1.5.\n",
    "\n",
    "> **NOTE**: Both input and reference data in the SPR dataset may contain NANs because observations or precipitation estimates may be missing or of insufficient quality. It is up to the user to handle those.\n",
    "\n",
    "We choose the ``on_swath`` geometry for the retrieval, which is a natural choice for pixel-based retrievals. We also set up batching in the dataset, which is more efficient for tabular data than leaving it to the PyTorch data loader to perform the batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30887ffe-c08d-4f9c-b8ba-9cf5edb61486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipwgml.input import GMI, Ancillary, Geo, GeoIR\n",
    "from ipwgml.target import TargetConfig\n",
    "\n",
    "target_config = TargetConfig(min_rqi=0.5)\n",
    "inputs = [GMI(normalize=\"minmax\", nan=-1.5, include_angles=False)]\n",
    "geometry = \"on_swath\"\n",
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e12e6b9-7076-45ea-a441-e076ccb3a152",
   "metadata": {},
   "source": [
    "With these settings, we can instantiate the training data dataset. By setting ``stack=True`` we also tell the retrieval to stack all input tensors instead of loading the input data as a dictionary.\n",
    "\n",
    "> **Note**: The current implementation of the dataset loads all training data into memory upon instantiation. Therefore, executing the cell below may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e679abf7-6dfc-421c-9c2d-f8eac00740e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from ipwgml.pytorch.datasets import SPRTabular\n",
    "\n",
    "training_data = SPRTabular(\n",
    "    reference_sensor=\"gmi\",\n",
    "    geometry=geometry,\n",
    "    split=\"training\",\n",
    "    retrieval_input=inputs,\n",
    "    batch_size=batch_size,\n",
    "    target_config=target_config,\n",
    "    stack=True,\n",
    "    download=True,\n",
    ")\n",
    "training_loader = DataLoader(training_data, shuffle=True, batch_size=None, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdef0ac-d153-407a-9131-0ae0ab6c44ed",
   "metadata": {},
   "source": [
    "With the above configuration, the training data loaded by the ``training_loader`` has the following dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98456697-5599-4030-b565-bf336f7d9315",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt, target = next(iter(training_loader))\n",
    "print(\"Input tensor shape: \", inpt.shape)\n",
    "print(\"Target tensor shape: \", target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479cc1bc-bbc5-4733-8142-05e2d8db204a",
   "metadata": {},
   "source": [
    "We also create a validation loader with the same configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5eda21-d394-4408-afb0-d107db88e8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = SPRTabular(\n",
    "    reference_sensor=\"gmi\",\n",
    "    geometry=\"on_swath\",\n",
    "    split=\"validation\",\n",
    "    retrieval_input=inputs,\n",
    "    batch_size=batch_size,\n",
    "    target_config=target_config,\n",
    "    stack=True,\n",
    "    download=True,\n",
    "    shuffle=False\n",
    ")\n",
    "validation_loader = DataLoader(validation_data, shuffle=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0279ec64-d5ab-4afe-a9d8-bed4c8a618a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits\n",
    "import lightning as L\n",
    "\n",
    "OUTPUTS = [\n",
    "    \"surface_precip\",\n",
    "    \"probability_of_precip\",\n",
    "    \"probability_of_heavy_precip\"\n",
    "]\n",
    "\n",
    "N_EPOCHS = 20\n",
    "\n",
    "class MLP(L.LightningModule):\n",
    "    \"\"\"\n",
    "    Lightning module implementing a multi-layer perceptron (MLP) for retrieving precipitation from satellite\n",
    "    observations.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input_features: int,\n",
    "        n_hidden_layers: int,\n",
    "        n_neurons: int,\n",
    "        activation_fn: Callable[[], nn.Module] = nn.GELU,\n",
    "        normalization_layer: Callable[[int], nn.Module] = nn.LayerNorm,\n",
    "        n_epochs: int = N_EPOCHS\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_input_features: The number of features in the input\n",
    "            n_hidden_layers: The number of hidden layers in the MLP\n",
    "            n_neurons: The number of neurons in the hidden layers\n",
    "            activation_fn: A callable to create activation function layers.\n",
    "            normalization_layer: A callable to create normalization layers.\n",
    "            n_epochs: The numebr of epochs the model will be trained for.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        blocks = [\n",
    "            nn.Linear(n_input_features, n_neurons),\n",
    "            normalization_layer(n_neurons),\n",
    "            activation_fn(),\n",
    "        ]\n",
    "        for _ in range(n_hidden_layers):\n",
    "            blocks += [\n",
    "                nn.Linear(n_neurons, n_neurons),\n",
    "                normalization_layer(n_neurons),\n",
    "                activation_fn(),\n",
    "            ]\n",
    "        self.body = nn.Sequential(*blocks)\n",
    "\n",
    "        heads = {}\n",
    "        for output in OUTPUTS:\n",
    "            heads[output] =  nn.Sequential(\n",
    "                nn.Linear(n_neurons, n_neurons),\n",
    "                normalization_layer(n_neurons),\n",
    "                activation_fn(),\n",
    "                nn.Linear(n_neurons, 1)\n",
    "            )\n",
    "        self.heads = nn.ModuleDict(heads)\n",
    "        self.n_epochs = N_EPOCHS\n",
    "\n",
    "    def forward(self, retrieval_input: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward retrieval input through network and produce dictionary with predictions.\n",
    "\n",
    "        Args:\n",
    "            retrieval_input: The retrieval input as a single torch.Tensor.\n",
    "\n",
    "        Return:\n",
    "            A dictionary containing the predictions for 'surface_precip', 'probability_of_precip',\n",
    "            and 'probability_of_heavy_precip'.\n",
    "        \"\"\"\n",
    "        y = self.body(retrieval_input)\n",
    "        return {\n",
    "            name: module(y) for name, module in self.heads.items()\n",
    "        }\n",
    "        \n",
    "    def training_step(self, batch, batch_idx) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculates the loss-function gradients for the MLP.\n",
    "\n",
    "        The loss is calculated as the sum of the MSE for 'surface_precip' and the binary cross-entropy loss\n",
    "        for precipitation detection and heavy precipitation detection.\n",
    "\n",
    "        Args:\n",
    "            batch: A tuple containing the training data loaded from the data loader.\n",
    "            batch_idx: The index of the batch in the current epoch. Not used.\n",
    "\n",
    "        Return:\n",
    "            A scalar torch.Tensor containing the total loss.\n",
    "        \"\"\"\n",
    "        inpt, surface_precip = batch\n",
    "        pred = self(inpt)\n",
    "\n",
    "        valid = torch.isfinite(surface_precip)\n",
    "        surface_precip = surface_precip[valid]\n",
    "        precip_mask = (surface_precip > 1e-3).to(dtype=torch.float32)\n",
    "        heavy_precip_mask = (surface_precip > 10).to(dtype=torch.float32)\n",
    "        surface_precip_pred = pred[\"surface_precip\"][valid]\n",
    "        pop = pred[\"probability_of_precip\"][valid]\n",
    "        pohp = pred[\"probability_of_heavy_precip\"][valid]\n",
    "        \n",
    "        # MSE loss for QPE\n",
    "        loss_quant = ((surface_precip_pred[..., 0] - surface_precip) ** 2).mean()\n",
    "        # BCE loss for detection targets\n",
    "        loss_detect = binary_cross_entropy_with_logits(pop[..., 0], precip_mask)\n",
    "        loss_detect_heavy = binary_cross_entropy_with_logits(pohp[..., 0], heavy_precip_mask)\n",
    "        tot_loss =  loss_quant + loss_detect + loss_detect_heavy\n",
    "        return tot_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx) -> None:\n",
    "        \"\"\"\n",
    "        Calculates the loss-function values on validation data.\n",
    "\n",
    "        Args:\n",
    "            batch: A tuple containing the training data loaded from the data loader.\n",
    "            batch_idx: The index of the batch in the current epoch. Not used.\n",
    "        \"\"\"\n",
    "        inpt, surface_precip = batch\n",
    "        pred = self(inpt)\n",
    "\n",
    "        valid = torch.isfinite(surface_precip)\n",
    "        surface_precip = surface_precip[valid]\n",
    "        precip_mask = (surface_precip > 1e-3).to(dtype=torch.float32)\n",
    "        heavy_precip_mask = (surface_precip > 10).to(dtype=torch.float32)\n",
    "        surface_precip_pred = pred[\"surface_precip\"][valid]\n",
    "        pop = pred[\"probability_of_precip\"][valid]\n",
    "        pohp = pred[\"probability_of_heavy_precip\"][valid]\n",
    "        \n",
    "        # MSE loss for QPE\n",
    "        loss_quant = ((surface_precip_pred[..., 0] - surface_precip) ** 2).mean()\n",
    "        # BCE loss for detection targets\n",
    "        loss_detect = binary_cross_entropy_with_logits(pop[..., 0], precip_mask)\n",
    "        loss_detect_heavy = binary_cross_entropy_with_logits(pohp[..., 0], heavy_precip_mask)\n",
    "        tot_loss =  loss_quant + loss_detect + loss_detect_heavy\n",
    "\n",
    "        opt = self.optimizers()\n",
    "        learning_rate = opt.param_groups[0]['lr']\n",
    "        \n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"val_loss\": loss_quant + loss_detect + loss_detect_heavy,\n",
    "                \"val_loss_quant\": loss_quant,\n",
    "                \"val_loss_detect\": loss_detect,\n",
    "                \"val_loss_detect_heavy\": loss_detect_heavy,\n",
    "                \"learning_rate\": learning_rate\n",
    "            },\n",
    "            on_epoch=True,\n",
    "            prog_bar=True\n",
    "        )\n",
    "    \n",
    "    def configure_optimizers(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        We use the Adam optimizer with a cosine annealing learning rate schedule.\n",
    "        \"\"\"\n",
    "        optimizer = optim.Adam(self.parameters(), lr=5e-4)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.n_epochs)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": scheduler\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e42aaba-5d99-40b9-81e1-1aa0e020f6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipwgml.input import calculate_input_features\n",
    "input_features = calculate_input_features(inputs, stack=True)\n",
    "mlp = MLP(n_input_features=input_features, n_hidden_layers=4, n_neurons=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abe1ccb-6fac-4e41-8613-684fd7317677",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(max_epochs=N_EPOCHS)\n",
    "trainer.fit(model=mlp, train_dataloaders=training_loader, val_dataloaders=validation_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31360065-cb8a-421e-bd58-4d61f4a01683",
   "metadata": {},
   "source": [
    "## Evaluating the retrieval\n",
    "\n",
    "In oder to evaluate the fully-connected precipitation retrieval, we need a function  to conventional precipitation retrievals, we can use the ``ipwgml.evaluation.Evaluator``. The evaluates the retrieval using the exact same data used to evaluate [the IMERG](evaluate_imerg.ipynb) and [GPROF](evaluate_gprof.ipynb) retrievals and thus ensures that the results are comparable. To ensure consistency between training and evaluation data, we instantiate the evaluator with the same values for ``geometry`` and ``retrieval_input`` as the training and validation dataset objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33ff035-fff2-4d35-ba80-bb1c04997aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipwgml.evaluation import Evaluator\n",
    "evaluator = Evaluator(\n",
    "    sensor=\"gmi\",\n",
    "    geometry=geometry,\n",
    "    retrieval_input=inputs,\n",
    "    download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997d11ad-ebb5-4f2b-8fb4-f90dbb9d97b6",
   "metadata": {},
   "source": [
    "## The retrieval callback function\n",
    "\n",
    "Evaluating the MLP using the ``ipwgml.evaluation.Evaluator`` requires implementing a retrieval callback function that the evaluator can call to obtain the rerieval results for a given collocation scene. The ``ipwgml.pytorch`` module provides a wrapper class that turns a given Pytorch retrieval into such a callback function. The ``PytorchRetrieval`` class simply takes the data from the evaluator and converts it to ``torch.Tensor`` objects and puts the results back into an ``xarray.Dataset``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3b25e4-57c6-4ad3-823a-d8aa0e79e404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipwgml.pytorch import PytorchRetrieval\n",
    "mlp_retrieval = PytorchRetrieval(mlp, retrieval_input=inputs, stack=True, device=torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f1aae8-02fc-4b3a-ac1d-c8399a20fe0b",
   "metadata": {},
   "source": [
    "## Case study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53652d20-6a50-49fc-a994-bf5613dc98bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = evaluator.plot_retrieval_results(86, mlp_retrieval, input_data_format=\"tabular\", batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7b3281-e29d-41b3-ab06-4a0640a7d3da",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feecab9b-0041-4d77-9f5b-fe6c9a55969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate(retrieval_fn=mlp_retrieval, input_data_format=\"tabular\", batch_size=4048, n_processes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8480a821-230b-4632-b463-cb29d75c7723",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b3facb-35b6-4bc4-b1a7-3a4bdbd1bc00",
   "metadata": {},
   "source": [
    "### Precipitation quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2539f82-732f-4f9a-8f8d-bff9bfe0943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.get_precip_quantification_results(name=\"MLP (GMI)\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c696d5e-fb67-4e03-97bb-7641ecaaa0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = evaluator.precip_quantification_metrics[-1].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3067ac9f-6a00-4378-90c3-054c7f7d8dfe",
   "metadata": {},
   "source": [
    "### Precipitation detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9719e4-4626-4a50-b21c-5d8eecfd62be",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.get_precip_detection_results(name=\"MLP (GMI)\").T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8414a5-d370-43e1-9e5f-ff348bb57422",
   "metadata": {},
   "source": [
    "### Probabilistic precipitation detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4222d60e-86f5-4beb-a035-d45864f848c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.get_prob_precip_detection_results(name=\"MLP (GMI)\").T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6249e574-f5a5-482c-81d7-6bffddaf1178",
   "metadata": {},
   "source": [
    "### Heavy precipitation detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5534f2f4-8f98-496a-9c05-5b7751da6c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.get_heavy_precip_detection_results(name=\"MLP (GMI)\").T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c73f1c4-0766-4942-b317-042d193a67e0",
   "metadata": {},
   "source": [
    "### Heavy probabilistic precipitation detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36093b3b-bb03-40cc-8074-4f71b0f8324e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.get_prob_heavy_precip_detection_results(name=\"MLP (GMI)\").T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
